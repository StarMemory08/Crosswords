from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from scrabble_env import ScrabbleEnv
import os

class TensorboardCallback(BaseCallback):
    def __init__(self, stop_timesteps, verbose=0):
        super(TensorboardCallback, self).__init__(verbose)
        self.stop_timesteps = stop_timesteps

    def _on_step(self) -> bool:
        self.logger.record("train/step", self.num_timesteps)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ timesteps ‡∏ñ‡∏∂‡∏á‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if self.num_timesteps >= self.stop_timesteps:
            print("‚úÖ ‡∏ñ‡∏∂‡∏á‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î timesteps ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")
            return False  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ó‡∏£‡∏ô
        
        return True  # ‡πÉ‡∏´‡πâ callback ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á `ScrabbleEnv`
env = ScrabbleEnv()

# ‚úÖ ‡πÉ‡∏ä‡πâ `PPO` ‡πÄ‡∏ó‡∏£‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
model = PPO(
    "MlpPolicy", env, verbose=1, tensorboard_log="./ppo_scrabble_logs/",
    n_steps=500, batch_size=50  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô PPO ‡∏£‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 100 timesteps
)

# ‚úÖ ‡πÉ‡∏ä‡πâ callback ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏´‡∏¢‡∏∏‡∏î
callback = TensorboardCallback(stop_timesteps=10)

# ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô + ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å TensorBoard
print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ó‡∏£‡∏ô PPO...")
model.learn(total_timesteps=10, reset_num_timesteps=False, progress_bar=True, callback=callback)
print(f"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! timesteps ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á: {model.num_timesteps}")

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
model.save("bot005")
print("üìÇ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
